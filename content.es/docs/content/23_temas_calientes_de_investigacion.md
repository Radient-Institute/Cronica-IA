---
weight: 23
bookFlatSection: true
title: "23 - Temas Calientes de Investigacion"
image: _din_style/banner_images/23_tcdi.webp
---

# Temas Calientes de Investigaci√≥n

Los modelos de lenguaje cambiaron por completo el campo de la IA. Aunque algunos detractores los catalogan como un *dead end* o un callej√≥n sin salida, la realidad es simple: mientras no aparezca una alternativa superior, van a seguir con nosotros. Son el tema central de la √©poca, y como era de esperarse, hay much√≠sima inversi√≥n enfocada en resolver sus limitaciones actuales.



## La ventana de contexto

El mecanismo de atenci√≥n escala cuadr√°ticamente con el n√∫mero de *tokens* de la secuencia, lo que hace que ventanas de contexto enormes sigan siendo inviables. Esto ha mejorado much√≠simo en los √∫ltimos a√±os, pasando de 8k en 2023 a un m√≠nimo de 256k en 2025, con algunos destacados como Google quien tiene 1M. Sin embargo, ingenieros del sector aseguran que con las t√©cnicas actuales podr√≠amos llegar a los 10M. Pero para alcanzar 100M necesitaremos un descubrimiento nuevo.

De este problema nace una l√≠nea de investigaci√≥n muy activa: buscar alternativas a la atenci√≥n tradicional. Casi todas las propuestas siguen la misma intuici√≥n: partir la secuencia, procesarla por secciones y mantener un estado. As√≠ aparecieron Hyena, RMKV, los *State Space Models*, Mamba, Titans y otros. Algunos de ellos permiten procesar secuencias indefinidamente gracias a este estado recurrente.

El inconveniente es que, seamos honestos, modelar lenguaje con cualquiera de estos algoritmos sigue siendo peor que con la vieja y confiable atenci√≥n‚Ä¶ que, por cierto, ya ha mutado en decenas de variantes: *multi-head latent attention*, *grouped-query attention*, *sparse attention*, entre muchas otras.

Esto nos deja con los llamados **modelos h√≠bridos**, que intercalan bloques con atenci√≥n lineal y bloques tradicionales. Se sacrifica algo de rendimiento, pero se gana escalabilidad.

Otro punto cr√≠tico, m√°s all√° del tama√±o de la ventana, es qu√© tan bien usa el modelo ese contexto. En 2024 apareci√≥ una startup llamada Magic.dev que afirmaba tener un modelo con 100M de contexto: **LTM2-mini**. Desde entonces desaparecieron casi por completo‚Ä¶ pero anunciaron una ronda de inversi√≥n de medio bill√≥n, lo cual deja claro el apetito del mercado: si descubres c√≥mo entrenar modelos con ventanas descomunales, tienes una startup valuada en cientos de millones instant√°neamente.


## Modelos de difusi√≥n para lenguaje

Una de las cr√≠ticas m√°s fuertes a los LLMs es su naturaleza autoregresiva: generan un *token* a la vez y, una vez escrito, no puede cambiarse. Esto, junto con la aleatoriedad del muestreo, puede causar errores que se propagan. Claro, el modelo puede generar m√°s texto para corregirlos, pero no es lo √≥ptimo.

Ante esto, los investigadores decidieron intentar **modelos de lenguaje dentro del marco de difusi√≥n**. Es decir, que la respuesta no nazca de forma secuencial, sino desde ruido, y que el modelo vaya refinando simult√°neamente todas las partes del texto. Primero un boceto, luego un refinamiento progresivo. Suena m√°s cercano a c√≥mo pensamos los humanos‚Ä¶ y adem√°s es much√≠simo m√°s r√°pido: para generar 1024 *tokens* ya no necesitas 1024 pasos, sino unas pocas decenas.

¬øEl problema?  
Que a√∫n est√°n muy por detr√°s de los LLMs tradicionales: entre dos y tres generaciones, dependiendo de a qui√©n preguntes. Funcionan, s√≠, pero usarlos se siente como viajar dos a√±os al pasado en calidad y cinco al futuro en velocidad.

El primer modelo decente de este tipo apareci√≥ en febrero del 2025: **LlaDA**, un 8B *open source*. Unos d√≠as despu√©s, la startup Inception Labs lanz√≥ acceso privado a su modelo propietario **MercuryCoder**. Y el √∫nico laboratorio frontera que mostr√≥ resultados con esta variante fue Google, que en abril public√≥ *benchmarks* de su **Gemini Diffusion**, a√∫n privado pero supuestamente comparable a Gemini 1.5 Flash.


## Cambiar el tokenizer

Otro punto de frustraci√≥n entre investigadores es el tokenizer: la forma en que se representa el texto antes de entrar al modelo. Siempre hay un *trade-off* entre tama√±o del vocabulario y longitud de la secuencia.

Las propuestas abundan:

- entrenar directamente sobre bits,  
- usar tokenizadores din√°micos,  
- tokenizar en UTF-8,  

pero de momento ninguna alternativa ha sido probada y validada a escala comercial.

## Continual learning

En octubre del 2025, Karpathy solt√≥ una de sus frases memorables:  

> ‚ÄúNo estamos construyendo animales; estamos invocando esp√≠ritus.‚Äù

Se refer√≠a a una de las limitaciones m√°s criticadas de los LLMs: **no aprenden de manera continua**.

Mientras el chat est√° activo, todo fluye. Pero abrir una conversaci√≥n nueva es empezar desde cero otra vez. Existen paliativos, como bases vectoriales o documentos que sirvan de memoria epis√≥dica, pero el aprendizaje continuo real sigue siendo un santo grial del campo.

## Modelos de energ√≠a

Sali√©ndonos del paradigma actual, est√°n los **modelos de energ√≠a**, popularizados por Hinton y LeCun. A diferencia de los modelos probabil√≠sticos, estos aprenden una funci√≥n que mide la afinidad entre un dato *x* y un dato *y*:  

> energ√≠a baja = compatibles,  
> energ√≠a alta = incompatibles.

Generar una respuesta *y* para una pregunta *x* consiste simplemente en navegar ese *mapa de energ√≠a* con descenso de gradiente hasta encontrar un punto de baja energ√≠a. El concepto es elegante, pero construir modelos frontera con este enfoque todav√≠a es un desaf√≠o abierto.

## Modelos del mundo

¬øQu√© es un ‚Äúmodelo del mundo‚Äù? Curiosamente, hace unos meses esta pregunta se volvi√≥ un meme en Twitter. Es una noci√≥n que parece intuitiva, pero cuesta verbalizarla.

Tomemos la definici√≥n de Schmidhuber:  
un modelo del mundo es **la representaci√≥n mental que construimos del entorno a partir de lo que percibimos**, una simplificaci√≥n del sistema real basada en conceptos y relaciones seleccionadas.

{{< image src="general_assets/world_model_comic.jpeg" alt="Modelo del mundo" title="Modelo del mundo" loading="lazy" class="img-small img-center" >}}

La pregunta es: *¬øc√≥mo construirlo en IA?*

Algunos creen que emerge autom√°ticamente en los LLMs. Otros apuestan por generadores de v√≠deo condicionados, como la serie **Genie**. Otros buscan obtenerlo mediante aprendizaje autosupervisado en im√°genes y v√≠deos, como Yan LeCun con las **JEPAs**, o Fei-Fei Li con sus modelos 3D.

Quiz√°s, solo quiz√°s, los modelos del mundo fueron los amigos que hicimos en el camino. üôÇ

---

Ilya Sutskever en una charla en la NeurIPS 2024 dijo que **la era del escalado del preentrenamiento hab√≠a llegado a su fin**. Lo compar√≥ con los combustibles f√≥siles: extremadamente √∫tiles, pero ya agotados.

Mostr√≥ una gr√°fica del volumen cerebral de los mam√≠feros en funci√≥n del tama√±o del cuerpo. Durante millones de a√±os sigui√≥ una ley de escala‚Ä¶ hasta que aparecieron los hom√≠nidos, cuya relaci√≥n rompi√≥ la tendencia con una pendiente mucho m√°s agresiva.

La naturaleza encontr√≥ una nueva manera de escalar en nosotros mismos. Nosotros tambi√©n debemos encontrarla.

Y en octubre del 2025, Ilya public√≥ un tuit diciendo que era ‚Äúel mejor d√≠a de su vida‚Äù. ¬øHabr√° encontrado algo realmente revolucionario?
