---
weight: 12
bookFlatSection: true
title: "12 - La Invasión De La Nación Del Centro"
image: _din_style/banner_images/12_lidc.webp
---

# La Invasión De La Nación Del Centro

## El Nuevo Abanderado

Era innegable que durante todo el 2024 China venía fuerte en el OSS, pero el momento en el que se apoderó por completo del espacio llegó recién en abril del 2025.

Después de que Mark Zuckerberg prometiera “el mejor modelo abierto del mundo” con Llama 4; y terminara entregandonos una serie de modelos densos, enormes y entrenados en los benchmarks para engañarnos; Alibaba Qwen quedó con la cancha completamente libre para volverse el estandar del *Open Source*. Qwen 2.5 ya era el modelo más usado para *finetunings* y experimentos en el nuevo paradigma del razonamiento. Sus resultados eran sólidos y generaban confianza.

La expectativa por la siguiente versión era enorme. Y en abril llegó **Qwen 3**: una familia de modelos densos y MoE desde 0.6B hasta 256B, híbridos al inicio y luego con variantes instruct y thinking puras. Sin engaños ni campañas de hype: simplemente una serie de modelos buenos que la comunidad adoptó de inmediato y agradeció.

Con esto, Llama había caído. Y con él, los últimos rastros de Occidente compitiendo seriamente en modelos abiertos durante el 2025.

OpenAI, viendo la desventaja, incluso intentó meterse en el OSS con GPT-OSS 120B y 20B. Aunque mostraban ser grandes razonadores en matemáticas y código, hay cosas importantes que mas alla de esos dos ambitos en los que estos modelos flaquean.


## El tridente en la frontera

Pocos saben que el mismo día que salió DeepSeek R1, otro laboratorio chino anunció que tenía un modelo de razonamiento comparable a O1: MoonshotAI presentó **Kimi K 1.5**. Pasó desapercibido porque solo publicaron un *technical report* y dieron acceso limitado, pero no se apresuraron porque su momento de fama llegaria meses despues.

En julio del 2025 lanzaron **Kimi K2**. Aunque lo describían oficialmente como un modelo “no razonador”, tenía una cantidad considerable de RL y *post-training* orientado a comportamiento agente. Rápidamente se colocó al nivel de los grandes: 1T de parámetros totales y 32B activos. Ahora si, con un lanzamiento como se debe: reporte técnico, modelo abierto y acceso gratuito desde su web. Llego a convertirse en el segundo DeepSeek.
La versión razonadora de K2 llegó en noviembre y fue igual de sorprendente, pero más potente.

Ese mismo julio apareció el tercer miembro del nuevo tridente chino: **GLM 4.5**, de Zhipu AI. Con 355B de parámetros totales y 32B activos, se volvió el favorito de muchos en tareas de código y uso de herramientas, compitiendo directamente con el rey indiscutible del área: Sonnet. Con una actualización posterior: GLM 4.6, mantuvieron su puesto en el podio.

Para este punto, DeepSeek comenzaba a quedarse atrás frente a sus primos menores. La comunidad pedía a gritos un DeepSeek R2.

Y llegó algo diferente: en diciembre del 2025, DeepSeek despertó con **DeepSeek V3.2**, un modelo que podía acercarse al poderoso Gemini 3 Pro de Google, pero con un precio absurdamente bajo: solo 42 centavos por millón de output tokens. También lanzaron **DeepSeek Math 2**, especializado en matemáticas.  
En su reporte admitieron abiertamente que sus modelos aún eran inferiores a los cerrados de Occidente en conocimiento del mundo, pero que planeaban resolverlo mediante una escala de preentrenamiento mucho mayor.

{{% hint info %}}
Hay otros dos laboratorios prometedores que sacaron modelos sumamente solidos: 

- **InclusionAI**, con *Ling-1T*,  
- **LongCat**, con *LongCat-Chat-Thinking* de 500B.  

Mientras tanto, los lanzamientos occidentales quedaron relegados a competir en tamaños intermedios: la serie Phi de Microsoft, Mistral 3 Large, sin embargo, tampoco lideraban esa categoría.
{{% /hint %}}

---

Es extraño pensar que, para finales del 2025, el top tres de modelos abiertos del mundo es íntegramente chino. Y que la familia estándar para investigación, personalización y experimentación también lo es. La Nación del Centro no solo ha entrado al mapa: ha tomado el mapa completo.
