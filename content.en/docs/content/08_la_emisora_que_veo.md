---
weight: 8
bookFlatSection: true
title: "08 – La Emisora Que Veo"
image: _din_style/banner_images/08_leqv.webp
---

# La Emisora Que Veo

I think that in recent AI history we haven’t seen a flex as big as Sora. It’s February 2024. After the success of AI image generation, there was massive interest in achieving something equivalent for video. Many *startups* were trying, and huge amounts of money were being invested.

Quality was improving slowly. The infamous *Will Smith eating spaghetti* from March 2023, generated with ModelScope, was already behind us. Videos were starting to look better, though many were little more than pans with slight motion. 2023 closed with **Runway** leading the *closed source* space with Gen-2, and **Stability AI** at the forefront of *open source* with Stable Video Diffusion.

Then, in February 2024, OpenAI dropped something absolutely shocking: **Sora**. It had illogical consistency, illogical quality, illogical actions within clips, and generations lasting up to a minute. Everything was illogical. They didn’t even call it a “video generator”—they presented it as a **“world generator.”**

How was it possible that a single lab had such an advantage? And let me stress this: video was not a neglected area. There was a lot of money and many teams working intensely in parallel.

The Sora effect was immediate: everyone took a step back to rethink goals and strategies. If there had been doubts about how far AI video could go, now we all knew this level of quality was possible. *Startups* and the *open source* ecosystem got to work.

Within a few months, we saw a wave of competitive launches.

{{% details title="Competitor launches" open=false %}}
- **Kling** in China (June)  
- **Luma AI** entering video with Dream Machine (June)  
- **Runway** announcing Gen-3 (June)  
- **PikaLabs v2** (July)  
{{% /details %}}

Even some large corporations felt compelled to show demos of things that clearly weren’t ready yet—like Google’s Veo 1 or Meta’s MovieGen—just to make it clear they “weren’t that far behind.”

Meanwhile, OSS advanced at breakneck speed thanks to techniques inherited from image generation: DiT, *flow matching*, *rectified flow*, and more. We got **Open Sora**, **LTX-Video**, **CogVideoX**, and as of now, **Wan 2.2** from Alibaba is probably the best the open ecosystem has to offer.

You might imagine OpenAI would use that time to dominate the market alone. But… no. Except for a few selected film directors, no one had real access to the original Sora model. Apparently, it was too large and too expensive to serve, but this was hidden behind the rhetoric of “safety” and the risks associated with generating video at that level of realism.

Ten months after the announcement, in December 2024, ChatGPT subscribers were given access to **sora-turbo**, a much weaker version. Where was the consistency? Where were the one-minute videos? Where was the quality? Well, there was some aesthetic appeal… but overall, sora-turbo was an outright failure.

The one who truly took all the glory was Google. They announced **Veo 3** in May 2025 during Google I/O: a massive leap forward, far superior quality, immediately available, with integrated audio. The king. Social media exploded. Memes, jokes, and viral videos made with Veo 3 flooded the internet. It was Google’s first truly viral AI product—and they won the race against OpenAI.

MidJourney joined the party later, in June, with its characteristic style and aesthetic. It’s astonishing to think that this bootstrapped *startup* is projecting 500 million dollars in revenue for 2025.

The rest of the year was marked by constant updates from all competitors in the video space: Kling, Hailuo, Luma, Runway… Even Google reinforced its leadership with **Veo 3.1**.

## Integrating AI Video into Audiovisual Content

AI-generated video has reached a level good enough to be consumed, but it still hasn’t fully integrated into all types of audiovisual content.

Cinema—feature films, series, documentaries—**has not adopted it yet**. The reasons could be many: small quality details, duration limitations, lack of fine-grained control in proprietary models that outperform OSS… or perhaps simply the inherent slowness of the film industry. Honestly, I don’t know.

YouTube has adopted it slightly, mostly as illustrative material. Where AI video truly found its refuge was in **short-form content**: TikTok, Reels, Shorts. Short enough to be generated with one or two *prompts*. Trivial enough that its mistakes don’t even warrant complaint.

## The Infinite *AI Slop* Machine

Short-form platforms filled up with AI videos. We knew that, at some point, someone would launch a social network exclusively for AI-generated video. The question was who—and when. The answer came from two unexpected places: **Meta and OpenAI**, almost in the same week, in September 2025.

Meta went first with **Vibes**, an AI video *feed* inside its Meta AI app. Since they didn’t have (and still don’t have) a competitive in-house video model, they decided to use MidJourney’s.

OpenAI, on the other hand, launched a full-fledged, standalone social network powered by the also-new **Sora 2**—a very strong model with a clear focus on social media content. Like Veo 3, it could generate audio and video together. But the feature that turned it into a viral phenomenon was **cameos**: anyone could scan themselves and ask Sora 2 to generate a video featuring them in any situation.

Within days of launch, the internet was flooded with videos of celebrities doing absurd things. It wasn’t impossible before—but now it was easier than ever… and free.

I have no further information about how Sora is doing, its user base, or the internal dynamics of that social network. I could dive into mega-labyrinths to bring you more information—but I’m not going dumpster diving.
